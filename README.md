# ğŸ“œ Smart Contract Summary & Q&A Assistant

![Python](https://img.shields.io/badge/Python-3.12+-blue)
![LangChain](https://img.shields.io/badge/LangChain-RAG-green)
![FAISS](https://img.shields.io/badge/VectorDB-FAISS-orange)
![Gradio](https://img.shields.io/badge/UI-Gradio-purple)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

An enterprise-ready **AI-powered legal assistant** built using **Retrieval-Augmented Generation (RAG)**.
The system enables users to upload contracts and ask contextual questions while ensuring:

* âœ… Grounded answers
* âœ… Strict source citations
* âœ… Minimal hallucination
* âœ… Modular & scalable architecture

---

# ğŸš€ Core Capabilities

## ğŸ“„ Document Processing

* PDF ingestion using PyMuPDF
* Context-aware recursive chunking
* Configurable chunk size & overlap

## ğŸ§  Retrieval-Augmented Generation (RAG)

* Embedding generation (SentenceTransformers)
* High-speed semantic retrieval via FAISS
* Top-k similarity search
* Context grounding before LLM inference

## ğŸ¤– LLM Integration

* GPT-4o-mini via GitHub Models API
* Prompt-engineered guardrails
* Deterministic answer formatting
* Fallback-ready architecture

## ğŸ›¡ï¸ Guardrails & Safety

* Context-only answering
* Automatic citation enforcement
* Hallucination mitigation
* Local vector storage (no external sharing)

## ğŸ–¥ï¸ Interactive UI

* Gradio-based modern web interface
* Upload & Chat separation
* Live chat history
* Source references displayed inline

---

# ğŸ—ï¸ System Architecture

## High-Level Architecture

```
User â†’ Gradio UI â†’ FastAPI Backend â†’ RAG Pipeline
                                      â†“
                              Vector Database (FAISS)
                                      â†“
                                   LLM API
```

---

## ğŸ”„ End-to-End System Flow

```mermaid
flowchart LR

    subgraph Frontend
        User["User (Upload & Ask)"]
        UI["Gradio UI\n(Chat + Sources)"]
    end

    subgraph Backend
        Ingestion["Ingestion Pipeline\n(Extract â†’ Chunk â†’ Embed)"]
        Retriever["Retriever\n(Semantic Similarity Search)"]
        LLMPipeline["LLM Pipeline\n(LangChain + Guardrails)"]
    end

    subgraph Storage
        VectorStore["Vector Store\n(FAISS Index)"]
    end

    %% Upload Flow
    User -- Upload PDF --> UI
    UI -- Process Document --> Ingestion
    Ingestion -- Store Embeddings --> VectorStore

    %% Question Flow
    User -- Ask Question --> UI
    UI --> Retriever
    Retriever -- Query top-k --> VectorStore
    VectorStore -- Relevant Chunks --> LLMPipeline
    Retriever --> LLMPipeline

    %% Response Flow
    LLMPipeline -- Answer + Citations --> UI
    UI -- Display Response --> User
```

---

# ğŸ› ï¸ Technology Stack

| Layer             | Technology                       |
| ----------------- | -------------------------------- |
| Language          | Python 3.12+                     |
| LLM Orchestration | LangChain                        |
| Vector Database   | FAISS                            |
| Embeddings        | HuggingFace SentenceTransformers |
| LLM Provider      | GPT-4o-mini (GitHub Models API)  |
| PDF Parsing       | PyMuPDF                          |
| UI                | Gradio                           |
| Backend           | FastAPI                          |

---

# âš™ï¸ Installation

## 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/yourusername/smart-contract-assistant.git
cd smart-contract-assistant
```

## 2ï¸âƒ£ Configure Environment

Create `.env` file:

```env
GITHUB_TOKEN="your_github_pat_here"
```

## 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

# â–¶ï¸ Running the Application

```bash
cd app
python ui.py
```

Then open the local URL provided by Gradio.

---

# ğŸ“‚ Project Structure

```
smart-contract-assistant/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”œâ”€â”€ vectorstore.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ ui.py
â”‚
â”œâ”€â”€ data/
â”œâ”€â”€ faiss_index/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# ğŸ“Š Performance Targets

| Metric            | Target                            |
| ----------------- | --------------------------------- |
| Response Time     | < 5 seconds                       |
| Retrieval Depth   | Top-3 / Top-5 chunks              |
| Embedding Latency | < 200ms per chunk                 |
| Index Build       | Linear scaling with document size |

---

# ğŸ“ˆ Evaluation Strategy

* Manual validation against contract clauses
* Retrieval accuracy testing
* Hallucination detection checks
* Top-k relevance scoring
* Response grounding verification

---

# ğŸ” Security Considerations

* Local FAISS storage
* No external document persistence
* API keys stored via environment variables
* Context-bound LLM responses

---

# ğŸ”„ Scalability & Production Readiness

* Modular pipeline architecture
* Swappable vector databases (FAISS / Chroma)
* LLM provider abstraction layer
* Fallback model support (local LLM)
* Docker-ready structure

---

# ğŸ§  Design Principles

* Grounded generation over free-form LLM output
* Separation of ingestion and inference
* Minimal token waste
* Extensible architecture
* Maintainable codebase

---

# ğŸš€ Future Enhancements

* Multi-document retrieval
* Contract auto-summarization
* Role-based access control
* Docker & Kubernetes deployment
* Evaluation dashboard
* CI/CD integration

---

# ğŸ“„ License

MIT License